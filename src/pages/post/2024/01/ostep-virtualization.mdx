---
title: OSTEP 学习笔记 —— Virtualization
date: 2024-01-31T20:29:41+08:00
image: virtual-peach.jpg
tags:
-   ostep
-   学习笔记
-   WIP
---

[《Operating Systems: Three Easy Pieces》](https://pages.cs.wisc.edu/~remzi/OSTEP/)第一部分 Virtualization 的学习笔记。

<Excerpt />

## A Dialogue on Virtualization

~~我觉得这个比喻很怪，peach 怎么能 virtualize，感觉不如举个别的例子。但挑 `og:image` 的时候想了想，这不，[桃 channel](https://www.bilibili.com/video/av55053935/) 吗（~~

## The Abstraction: The Process

所谓 <dfn>process</dfn>，就是 *a running program*。

一个 process 的 machine state 包括 memory、register（包括 PC、stack pointer 等）、I/O information（例如打开的文件列表）等。

在创建 process 时，OS 需要 (lazy) load program code 和 data，初始化 stack 和 heap，设置 `argc`、`argv`，设置 `stdin`、`stdout`、`stderr` 三个 file descriptor。

一个 process 有三种 state：running、blocked、ready。

```mermaid
flowchart LR
    Running -- "initiate I/O" --> Blocked -- "finish I/O" --> Ready
    Ready -- schedule --> Running -- deschedule --> Ready
```

OS 的 scheduler 需要决定如何调度 process state，以优化性能。例如，一个 process initiate I/O 后应当 schedule 到另一个 ready 的 process。

OS 需要维护 process list，记录 process memory address、kernel stack address、register context、process state、pid、parent、killed、opened files、cwd、trap frame 等信息。

## Interlude: Process API

`fork()`、`wait()`、`exec()` 以及 signals 参见 [CS:APP 第八章](/post/2022/11/csapp-8#process-control)。

`fork()` 和 `exec()` 通常配合使用，而被设计成了分离的两个 API，所以可以在它们之间插入其他代码，以修改 child process 的执行环境。例如，在 shell 中执行命令可以创建 child process 然后 wait，如果需要 redirect output，可以在 `fork()` 和 `exec()` 之间执行 `close()` 和 `open()`。

<Card title="A <code>fork()</code> in the road">

-   论文：[A fork() in the road](https://www.microsoft.com/en-us/research/uploads/prod/2019/04/fork-hotos19.pdf)
-   相关讨论：[LWN](https://lwn.net/Articles/785430/)、[Reddit](https://www.reddit.com/r/programming/comments/bbic2e/a_fork_in_the_road/)、[Hacker News](https://news.ycombinator.com/item?id=19621799)

`fork()` 的设计有很多缺点：新功能的设计需要考虑到 `fork()`（新的状态如何复制）而变得复杂，有时需要特殊修改软件来适配 `fork()`（例如 buffered I/O 在 fork 前需要 flush），有的硬件（例如 GPU）不支持复制状态，`fork()` 不 thread-safe，将所有信息共享给 child process 可能泄露信息，保持 memory layout 不变可能导致 ASLR 失效，不必要地复制整个 address space 会影响性能，不管剩余 RAM 有多少就分配 copy-on-write memory（overcommit）可能导致 OOM，支持 copy-on-write 会鼓励 monolith kernel……

`fork()` 历史悠久，使用广泛，如果 OS 不实现则会使得很多程序无法运行。但在理论上，`fork()` 可能可以被替换：

-   `fork()` + `exec()` 创建 child process 可以替换为合二为一的 (`posix_`)`spawn()`。
-   `spawn()` 的参数可能不够灵活，可以添加 cross-process operation 来代替位于 `fork()` 和 `exec()` 之间的任意操作。（但这样的替代可能不够简洁直观。）
-   使用 `fork()` 来实现 multi-process 可以替换为 multi-thread，或者创建全新的 child process。（但这样的话就不能共享初始化，可能会有一定的性能损失。）
-   可以增加新的 API 来代替使用 `fork()` 获取 copy-on-write memory。（可能仍会面临 overcommit 的问题。）
-   对于少量仍需使用 `fork()` 而性能要求不高的场景，或者是为了兼容使用 `fork()` 的软件，可以抛弃 low-level 的 `fork()` 而在 high-level（低效地）实现这一功能。

</Card>

## Mechanism: Limited Direct Execution

“direct execution” 就是直接执行一个 program，但这样做无法对 user program 进行限制，例如可能访问包括 kernel memory、其他 process 在内的任意 memory、一直运行而不把 control 交给 OS。所以，OS 需要采用 *limited direct execution*，对 process 施加限制。这样做虽然 “limited”，但依然是 CPU 直接执行 user program instruction，所以不会有太多的 overhead。

### Problem #1: Restricted Operations

为了限制 user program 的行为，CPU 的执行分为 *user mode* 和 *kernel mode*。kernel mode 具有更高的权限，例如可以直接访问 memory、执行 I/O。

user program 需要通过 *system call* 来进入 kernel mode，由 OS 执行相应的操作。system call 是一种特殊的 *trap*（exception），通过 trap instruction 进入 *trap handler* 并把 register 等状态存下来，操作执行完毕后再 return-from-trap 回到 user program 调用 system call 之后的位置并恢复 register 等状态。

在系统启动时，OS 会设置 *trap table*，即各种 trap 对应的 handler address。*system-call number*（放在特定 register 或 stack 特定位置）用来指定要执行哪个 system call。trap table 只能由 OS 设定，以避免 user program 任意指定 kernel mode 下跳转到的位置。

### Problem #2: Switching Between Processes

在一个 process 占用着 CPU 时，OS 没有运行，自然无法实现 control，所以需要 user program 把 control 交给 OS。这有两种方式，一种是 cooperative approach，即调用 system call；一种是 non-cooperative approach，即使用 timer interrupt，每隔一段时间就把 control 强制交给 OS，以避免单个 process 连续运行过长时间甚至进入死循环而只能重启。

决定了要切换 process 时，OS 会进行 *context switch*，主要操作是从 process A 的 registers 和 kernel stack 切换到 process B 的 registers 和 kernel stack，之后 return-from-trap 时就会返回到 process B 之前离开的地方。

## Scheduling: Introduction

OS scheduler 需要决定 schedule 到哪个 process，这表现为 *scheduling policy* (*discipline*)。

### Workload Assumptions

在这一部分，我们先对 *workload*，即需要运行的 processes（即 jobs），作一些（不切实际的）assumption 以简化问题，后面再逐步丢弃这些 assumption：

1.  每个 job 用时相同。
2.  每个 job 同时 arrive。
3.  每个 job 一旦开始就一直运行到结束，不被打断。
4.  每个 job 都只使用 CPU，不使用 I/O（没有 system call，不会 blocked）。
5.  每个 job 的用时是已知的。

其中，4 和 5 是最不切实际的：没有 I/O 的 program 运行了没有任何意义；scheduler 无法预知 job 要运行多久（连是否停机都无法预知）。

### Turnaround Time

*turnaround time* 是一个 *scheduling metric*，它指的是一个 job 从 arrival 到 completion 的用时，用来衡量总体性能。

FIFO (first in first out / FCFS, first come first served) 是一种最简单的 scheduling policy。在所有 5 个 assumption 下，任何 scheduling policy 都是一样的，FIFO 就可以达到最优。

如果丢弃 assumption 1 而继续使用 FIFO，当排在最前的 job 用时很长时，会造成 *convoy effect*，堵住后面用时短的其他 job，使得 turnaround time 变得很大。此时，可以采用 SJF (shortest job first) 达到最优解。

如果进一步丢弃 assumption 2，有可能最长的 job 最先到，短的 job 紧随其后，SJF 就失效了。此时，需要再丢弃 assumption 3，来允许 scheduler *preempt* 一个 job 而 schedule 到另一个（不这样做的 scheduler 被称作 *non-preemptive* scheduler），然后就可以采用 STCF (shortest time-to-completion first / PSJF, preemptive shortest job first) 达到最优解：在新 job arrive 时，如果它的总用时比当前 job 的剩余用时还短，可以 schedule 到新 job。

### Response Time

为了让用户在交互中获得更好的体验，turnaround time 是不够的，还需要引入新的 metric，*response time*，它可以用一个 job 从 arrival 到 first run 的用时衡量。

上面提到的各种 scheduling policy，例如 STCF，response time 都很差，被排到后面运行的 job 需要等待很久。

Round-Robin (RR) policy 会让每个 job 运行一个 *time slice* (*scheduling quantum*)，然后切换到下一个 job，所以 RR 也被称作 *time-slicing*。time slice 越小 response time 也就越小，但如果 time slice 过小，context switching（包括存储/恢复 register，以及 cache miss penalty）在用时中的占比就会过大，从而显著影响性能，所以需要一定大小的 time slice 来 *amortize* 掉 context switching cost。RR 的 response time 较小，但 turnaround time 很大，比 FIFO 还大。

一般来说，如果一个 policy 是 *fair* 的，均等地将 CPU 分配给各个 job，就会有较差的 turnaround time 和较好的 response time；如果一个 policy unfair，就可以有较好的 turnaround time，但 response time 会较大。这是一个固有的 trade-off：鱼与熊掌，不可兼得。[^cake]

[^cake]: 作者推荐阅读：[You can't have your cake and eat it - Wikipedia](https://en.wikipedia.org/wiki/You_can%27t_have_your_cake_and_eat_it)  
         “The best part of this page is reading all the similar idioms from other languages.”

### Incorporating I/O

如果进一步丢弃 assumption 4，即允许 job 进行 I/O，则需要处理 blocked 的情况。

一般来说，可以将一个 job 视作被 I/O 分割成的多个 sub-job，然后按照之前的 policy 进行 schedule。例如，使用 STCF 时，在 sub-job 的视角下，会优先运行 I/O 密集的 job，这可以达成 *overlap*，让 CPU 和 I/O 同时工作，更加充分地利用系统资源。

## Scheduling: The Multi-Level Feedback Queue

之前这些简单的 scheduling policy 面临两大问题，一是 turnaround time 和 response time 之间的矛盾，二是 SJF/STCF 对 perfect knowledge（assumption 5）的依赖。

Multi-level Feed-back Queue (MLFQ) 是目前被广泛使用的一种 scheduling policy，同时解决了这两大问题。

### Basic Rules

MLFQ 的基本思路是，workload 可以分为两类，一类是 short-running interactive job（被 I/O 切成了小块），一类是 long-running CPU-intensive job。interactive job 更需要优先运行，这既是 SJF/STCF 的基本思想，同时也是因为 interactive job 对 response time 的要求更高。

MLFQ 的基本运行规则为：有多个不同 priority 的 job queue，每次会选择 priority 最高的 queue，在同一个 queue 内使用 RR。

理想情况下，interactive job 的 priority 会更高，从而逼近 STCF。

### Priority Adjustment

因为 scheduler 无法预先知道 job 的类型，priority 是根据程序的运行情况动态设置的。

job arrive 时会先放在 priority 最高的 queue，如果运行太久就会降低 priority。具体来说，一个 job 在每一级 queue 会获得一段 *time allotment*，在这一级 queue 的累计运行时长如果超过 allotment 就会降低 priority 到下一级 queue。

在这样的机制下，会有两个问题：

-   CPU 可能被几个 interactive job 占满，导致 priority 低的 long-running job 一点 CPU 都拿不到（这被称作 *starvation*）。
-   一个 job 的行为可能随时间变化，如果经过一段 CPU-intensive 后 priority 到了最低，然后变为 interactive，priority 无法恢复。

为了解决这两个问题，可以每隔一段时间进行 *priority boost*，将所有 job 的 priority 设为最高。

<Card title="Game the scheduler">

如果 allotment 不是计算累计用时，而是每次运行时单独判断这次有没有到一个 threshold，没达到就不降低甚至提高 priority，是否可以达到同样的效果，甚至自动检测到 long-running 转变为 short-running 而提升 priority？

答案是，这样会允许一个 program 恶意地 *game the scheduler*，每次都恰好在 threshold 之前一点点执行 I/O，从而在长时间占用 CPU 的同时维持最高的 priority，最终达到接近独占 CPU 的效果。这在多个用户共享 CPU 时（例如在云服务器上）可能造成安全问题。

</Card>

### Tuning MLFQ

MLFQ 有很多可以设置的参数：queue (level) 的数量，每个 level 的 time slice 和 allotment，多久进行一次 priority boost。一般来说，priority 越高，time slice 和 allotment 越长。

MLFQ 不一定真的要实现为多个 queue，也可以统计每个 job 的 CPU usage，根据 usage 计算出 priority，而让 usage 随时间 *decay* 来代替 priority boost，这称作 *decay-usage* scheduling。

priority 不一定要完全基于 feedback，也可以参考由用户提供的 *advice*，例如使用 `nice` 命令可以设置 *niceness* 来影响 job priority。

## Scheduling: Proportional Share

这章讨论的是一种不同的 scheduler，它的主要目标是按一定的比例将 CPU 分配给各个 job。例如，在 virtualized data center / cloud 中，可以将 CPU 均等地分配给各个用户。

### Lottery Scheduling

在 *lottery scheduling* 中，每个 job 被分配了一定数量的 *ticket*，每个 time slice 结束时随机选择一个 winning ticket，schedule 到对应的 job，时间久了之后会趋近于按 ticket 数量的 CPU 分配。

lottery scheduling 还会提供一些 ticket mechanism：

-   可以给每个 user 一些 ticket，每个 user 再将 ticket 分给各个 job。
-   一个 job 可以将自己的 ticket transfer 给其他 job。
-   如果各个 job 之间互相信任，可以进行 ticket inflation，一个 job 需要更多 CPU 时直接给自己更多 ticket 即可，不需要和其他 job 沟通。

### Stride Scheduling

stride scheduling 不依赖于随机，可以确定性地达到设定的比例。

用一个大数除以每个 job 的 ticket value 得到每个 job 的 *stride*，对每个 job 维护一个 *pass*，每次运行一个 job 后将 pass 加上 stride，schedule 到 pass 最低的 job。每个循环内都会精确地达到设定的比例。

stride scheduling 的一个劣势是新加入的 job 的 pass 不好设定，而 lottery scheduling 不需要维护状态，可以轻松地添加新的 job。

### The Linux Completely Fair Scheduler (CFS)

Linux 的 Completely Fair Scheduler (CFS) 高效而 scalable 地实现了 fair-share scheduling。

CFS 会记录每个 job 的 *virtual runtime* (`vruntime`)，每次运行后加上这次运行的时长，schedule 到 `vruntime` 最低的 job。time slice 是不固定的，由 `sched_latency` 除以 job 数量决定，并会与 `min_granularity` 取较大值，防止 time slice 过小导致 context switching 过多。time slice 可能不是 timer interrupt 的整数倍，但 `vruntime` 会精确记录实际用时。

可以通过设置 `niceness` 调整一个 job 的 `weight`，`niceness` 越低 `weight` 越高，呈指数降低的关系。设置了 `niceness` 后，time slice 会按照 `weight` 分配，而 `vruntime` 每次增加的值会再调整回来，即 `weight` 大的会获得更大的 time slice 而增长同样多的 `vruntime`。

CFS 使用 red-black tree 维护各个 job 的 `vruntime` 以快速取出最小值。一个 job blocked 时会从树上移除，而再插入到树上时其 `vruntime` 会设为此时树上的最小值，以避免一个长期 blocked 的 job 恢复后独占 CPU，但这样会导致 I/O 频繁或长期 sleep 的 job 实际上没有拿到 fair share。

作为一个得到了广泛实际应用的 scheduler，CFS 还有很多其他 feature，例如通过一些手段优化了 cache performance，可以高效处理有多个 CPU 的情形，可以将多个 process 视作一个 group 而不是对每单个 process 进行 schedule。
